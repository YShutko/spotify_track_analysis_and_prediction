{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab52d509",
   "metadata": {},
   "source": [
    "# Spotify Track Analytics Popularity Prediction: ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c314833",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9582f7a8",
   "metadata": {},
   "source": [
    "This notebook extends the Spotify track exploration work by building end-to-end ML workflows that predict a song's popularity score from its audio features and metadata. We will establish a clean dataset, prepare modeling pipelines, and compare baseline algorithms to understand which inputs drive popularity predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccda000",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd498c74",
   "metadata": {},
   "source": [
    "For establishing furhter ML modeling, it is necessary to import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 #import pandas for data manipulation\n",
    "import numpy as np             #import numpy for numerical operations\n",
    "import seaborn as sns       #import seaborn for data visualization\n",
    "import matplotlib.pyplot as plt                 #import matplotlib for plotting\n",
    "from sklearn.model_selection import train_test_split  #import train_test_split for splitting data                                                      \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder #import preprocessing tools\n",
    "from sklearn.compose import ColumnTransformer #import ColumnTransformer for preprocessing\n",
    "from sklearn.pipeline import Pipeline                  # import Pipeline for creating ML pipelines\n",
    "from sklearn.linear_model import LinearRegression   #import Linear Regression model\n",
    "from sklearn.ensemble import RandomForestRegressor  #import Random Forest Regressor\n",
    "from xgboost import XGBRegressor  #import XGBoost Regressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  #import evaluation metrics\n",
    "from sklearn.preprocessing import StandardScaler  #import StandardScaler for feature scaling\n",
    "from sklearn.decomposition import PCA  #import PCA for dimensionality reduction\n",
    "from sklearn.cluster import KMeans   #import KMeans for clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce21b8",
   "metadata": {},
   "source": [
    "As the first step the cleaned in [previous notebook](https://github.com/YShutko/CI_spotify_track_analysis/blob/2972bbfa3227c3ab665618cfcf3cae74f5215dbe/notebooks/Spotify_track_analysis.ipynb) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221f3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/spotify_cleaned_data.csv')  # Adjust path as needed\n",
    "df.head ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af842754",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e4fd2",
   "metadata": {},
   "source": [
    "### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92e450",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695cfe55",
   "metadata": {},
   "source": [
    "In this part of the project, we build a Linear Regression model to predict a track’s popularity based on its audio features (danceability, energy, loudness, valence, etc.) and engineered features (mood, intensity). Linear Regression helps us understand how each feature influences popularity and serves as a simple baseline model. Before training, we preprocess the data using scaling and one-hot encoding to ensure all features are on comparable scales and properly formatted for the model. This baseline evaluation provides a reference point for comparing more advanced machine learning models later in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f16b5",
   "metadata": {},
   "source": [
    "#### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee22b6",
   "metadata": {},
   "source": [
    "In the following steps feature engeneering is performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00597b",
   "metadata": {},
   "source": [
    "Linear regression can only learn straight-line relationships.\n",
    "But many musical characteristics are nonlinear and interact with each other.\n",
    "By multiplying two features, we allow the model to learn interaction patterns that linear regression cannot capture naturally.\n",
    "\n",
    "Energy * loudness helps the model understand: \"How intense, powerful, and high-impact the song feels.\"\n",
    "This is strongly related to genre and mood, which correlate with popularity and listener engagement.\n",
    "Without this feature:\n",
    "The model only sees energy and loudness independently\n",
    "→ losing important relationships\n",
    "→ decreasing predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933295db",
   "metadata": {},
   "source": [
    "\"Mood\" is not directly in the dataset, but it is one of the strongest predictors of human music preference.\n",
    "These 3 features measure:\n",
    "* Danceability → how rhythmically engaging\n",
    "* Energy → how active or intense\n",
    "* Valence → positivity/happiness\n",
    "Averaging them gives a general emotional / vibe score.\n",
    "What it gives the model:\n",
    "* A single, compact representation of “how the song feels”\n",
    "* Helps cluster songs by vibe\n",
    "* Helps regression capture patterns like:\n",
    "* happy energetic songs tend to be more popular\n",
    "* sad low-energy songs tend to rank differently\n",
    "\n",
    "Why linear regression benefits:\n",
    "This creates a latent feature (a hidden variable) that captures something meaningful which raw variables alone cannot represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac10197",
   "metadata": {},
   "source": [
    "Spotify dataset stores \"explicit\" as 0/1 integers, which machine learning treats as numeric.\n",
    "But it is not numeric — it’s categorical (yes/no).\n",
    "\n",
    "If not converted:\n",
    "* The model will treat explicit = 1 as “higher” than explicit = 0\n",
    "* It will try to fit a line like: popularity = ... + 3.7 * explicit\n",
    "* This is incorrect, because explicitness is not a quantity\n",
    "\n",
    "After converting to Boolean:\n",
    "* True / False becomes a category\n",
    "* It will be one-hot encoded correctly (explicit_True, explicit_False)\n",
    "\n",
    "This avoids incorrect numerical influence and improves model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction: energy * loudness (how intense & loud a track feels)\n",
    "df[\"energy_loudness\"] = df[\"energy\"] * df[\"loudness\"]\n",
    "\n",
    "# Simple \"mood\" score: combination of danceability, energy, valence\n",
    "df[\"mood\"] = (df[\"danceability\"] + df[\"energy\"] + df[\"valence\"]) / 3\n",
    "\n",
    "#explicit is not boolean already, convert it\n",
    "df[\"explicit\"] = df[\"explicit\"].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf05f06",
   "metadata": {},
   "source": [
    "Next step is to select the prediction target and features.\n",
    "`popularity` is used as the target variable `y`. `X` is builded from sound characteristics (tempo, danceability, energy, etc.), boolean/context flags (explicit, key, mode), and any engineered aggregates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"popularity\"\n",
    "\n",
    "numeric_features = [\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"key\",\n",
    "    \"loudness\",\n",
    "    \"mode\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\",\n",
    "    \"time_signature\",\n",
    "    # engineered:\n",
    "    \"duration_min\",\n",
    "    \"energy_loudness\",\n",
    "    \"mood\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"explicit\",      # True/False\n",
    "    \"track_genre\",   # 125 genres, will be one-hot encoded\n",
    "]\n",
    "\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb26564",
   "metadata": {},
   "source": [
    "Preprocessing: Scaling + One-Hot Encoding\n",
    "\n",
    "StandardScaler\n",
    "Scales all numeric features so they have mean = 0 and standard deviation = 1.\n",
    "This prevents large-valued columns (e.g., duration_ms) from dominating the model.\n",
    "\n",
    "OneHotEncoder\n",
    "Converts categorical features (e.g., track_genre, explicit) into numeric vectors.\n",
    "handle_unknown=\"ignore\" prevents errors if unseen categories appear in test data.\n",
    "\n",
    "ColumnTransformer\n",
    "Applies scaling to numeric columns and one-hot encoding to categorical columns in a single unified preprocessing step.\n",
    "Ensures consistent, leak-free preprocessing inside the ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: scaling + one-hot encoding\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1971f",
   "metadata": {},
   "source": [
    "Pipeline: Preprocessing → Linear Regression\n",
    "\n",
    "The Pipeline combines all steps into one model:\n",
    "\n",
    "First, it applies preprocessing\n",
    "(scaling numeric features + one-hot encoding categorical features)\n",
    "\n",
    "Then, it runs LinearRegression on the transformed data.\n",
    "\n",
    "This ensures the same preprocessing is applied during both training and prediction, preventing data leakage and keeping the workflow clean and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fcdfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build pipeline: preprocessing → linear regression\n",
    "lr= Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"regressor\", LinearRegression())\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb6ac1",
   "metadata": {},
   "source": [
    "Train/Test Split\n",
    "train_test_split divides the dataset into two parts:\n",
    "* Training set (80%) → used to fit the model\n",
    "* Test set (20%) → used to evaluate how well the model generalizes\n",
    "\n",
    "random_state=42 ensures the split is reproducible every time you run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fda87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0265351e",
   "metadata": {},
   "source": [
    "fit the Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f7a03",
   "metadata": {},
   "source": [
    "And save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('../model/', exist_ok=True)  # Create directory if it doesn't exist\n",
    "# joblib.dump(lr, '../model/spotify_lr.pkl') # Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d5feb",
   "metadata": {},
   "source": [
    "Model Evaluation\n",
    "After predicting values on the test set (y_pred), we calculate key regression metrics:\n",
    "* MAE (Mean Absolute Error): Average absolute difference between predicted and actual popularity.\n",
    "* RMSE (Root Mean Squared Error): Penalizes large errors more strongly; shows overall prediction error.\n",
    "* R² Score: Measures how much variance in popularity the model explains (1 = perfect fit, 0 = no predictive power).\n",
    "\n",
    "These metrics tell us how well the linear regression model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Linear Regression results:\")\n",
    "print(f\"MAE  : {mae:.3f}\")\n",
    "print(f\"RMSE : {rmse:.3f}\")\n",
    "print(f\"R^2  : {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3bb0e",
   "metadata": {},
   "source": [
    "* R² = 0.25 means:Weak model fit\n",
    "Linear relationships between features and target are limited\n",
    "Model captures only basic trends (e.g., louder = slightly more popular)\n",
    "\n",
    "* MAE ≈ 14 → Not acceptable for predictive tasks:\n",
    "A good model should have MAE < 8 for Spotify popularity\n",
    "Industry models often get MAE ≈ 6–10 by adding external metadata\n",
    "\n",
    "* RMSE ≈ 19 → High variance in the errors:\n",
    "The model makes some very large mistakes\n",
    "Linear Regression underfits the data\n",
    "Popularity is nonlinear + influenced by many missing factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51649c5",
   "metadata": {},
   "source": [
    "Numeric feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names from the preprocessing step\n",
    "feature_names = lr.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "\n",
    "# Extract coefficients from the linear regression model\n",
    "coefficients = lr.named_steps[\"regressor\"].coef_\n",
    "\n",
    "# Build importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coefficient\": coefficients,\n",
    "    \"abs_importance\": np.abs(coefficients)\n",
    "}).sort_values(\"abs_importance\", ascending=False)\n",
    "\n",
    "numeric_importance = importance_df[\n",
    "    ~importance_df[\"feature\"].str.contains(\"track_genre\")\n",
    "]\n",
    "\n",
    "numeric_importance.head(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da224fbc",
   "metadata": {},
   "source": [
    "Top Predictors:\n",
    "* explicit (True/False) → explicit content has the strongest effect among numeric/categorical non-genre features.\n",
    "    * Negative/positive signs cancel because both dummy columns appear; importance is the magnitude.\n",
    "* danceability → more danceable tracks tend to be more popular.\n",
    "* valence → happier, more positive-sounding songs tend to be more popular.\n",
    "* acousticness (negative) → high acousticness lowers predicted popularity.\n",
    "* mode and energy → moderately contribute; higher energy slightly increases popularity.\n",
    "* speechiness (negative) → tracks with more spoken content (rap, spoken word) tend to score lower.\n",
    "* mood → higher combined mood (danceability + energy + valence) improves popularity.\n",
    "* energy_loudness → intensity contributes but less than basic features.\n",
    "\n",
    "Lower-Influence Features\n",
    "* instrumentalness → instrumental tracks slightly decrease popularity.\n",
    "* time_signature, tempo, loudness, liveness → very small effect.\n",
    "* key, duration_min → minimal influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_num = numeric_importance.head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_num[\"feature\"], top_num[\"abs_importance\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 20 Numeric Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1427ba",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "* Popularity is influenced more by overall vibe (danceability, valence, energy) than by technical music properties (tempo, key, duration).\n",
    "* Explicit content and genre dominate, but once genres are removed, the model relies mainly on danceability, positivity, and acousticness.\n",
    "* Many audio features contribute only weakly, explaining why the linear regression model achieves low R²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070100f",
   "metadata": {},
   "source": [
    "Top 20 genres from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3885d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_importance = importance_df[\n",
    "    importance_df[\"feature\"].str.contains(\"track_genre\")\n",
    "]\n",
    "\n",
    "genre_importance.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef76fcdc",
   "metadata": {},
   "source": [
    "Genres with Strong Negative Impact on Popularity.\n",
    "\n",
    "These genres have large negative coefficients, meaning being in this genre reduces predicted popularity:\n",
    "* iranian (strongest negative)\n",
    "* romance\n",
    "* pop film\n",
    "* latin\n",
    "* detroit techno\n",
    "* chicago house\n",
    "* jazz\n",
    "* chill\n",
    "* kids\n",
    "\n",
    "These may have smaller listener bases or niche audience groups in the dataset, leading to lower average popularity.\n",
    "\n",
    "Genres with large positive coefficients contribute to higher popularity predictions:\n",
    "* k-pop (largest positive)\n",
    "* sad\n",
    "* indian\n",
    "* country\n",
    "* grunge\n",
    "* sertanejo\n",
    "* anime\n",
    "\n",
    "These styles have broad, dedicated fanbases or strong cultural streaming support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eeb15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genres = genre_importance.head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_genres[\"feature\"], top_genres[\"abs_importance\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 20 Genres Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c8c42",
   "metadata": {},
   "source": [
    "Key insights:\n",
    "* Genre is more important than audio features for predicting popularity in this dataset.\n",
    "* The model is essentially learning each genre’s average popularity level.\n",
    "* This is common in Linear Regression, because genre directly encodes who listens to the music, not just how it sounds.\n",
    "\n",
    "Why Linear Regression behaves this way\n",
    "* One-hot encoded genres act like labels for entire listener communities.\n",
    "* Popularity varies drastically across communities (e.g., k-pop vs. techno).\n",
    "* These differences produce much larger coefficients than scaled numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168ef4a",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418685b",
   "metadata": {},
   "source": [
    "The linear regression baseline offers a transparent look at how each standardized feature impacts predicted popularity, making it ideal for initial diagnostics. However, any systematic under/over-prediction patterns or mediocre MAE/R^2 results highlight the need for richer models (tree ensembles, gradient boosting) and potentially more expressive feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493f9a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc5653",
   "metadata": {},
   "source": [
    "#### What to Do Next to Improve the Model\n",
    "1. Switch from Linear Regression → Tree-based models\n",
    "They outperform linear models for this dataset:\n",
    "* RandomForestRegressor\n",
    "* XGBoost\n",
    "* GradientBoostingRegressor\n",
    "* LightGBM\n",
    "\n",
    "They capture nonlinear relationships.\n",
    "\n",
    "2. Add powerful derived features\n",
    "* Track age (release_year)\n",
    "* Artist popularity\n",
    "* Genre grouped into macro-genres\n",
    "* Energy × Valence (mood)\n",
    "* Loudness × Danceability\n",
    "\n",
    "3. Remove low-variance or useless features (key, mode, time_signature barely matter)\n",
    "\n",
    "4. Try a classification approach\n",
    "Predict popularity classes instead of exact numbers:\n",
    "* 0–40 = low\n",
    "* 41–70 = medium\n",
    "* 71–100 = high\n",
    "Classification performs better than regression on music data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9a98c7",
   "metadata": {},
   "source": [
    "#### 2. Tree based models (RandomForest, XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511491c5",
   "metadata": {},
   "source": [
    "##### 2.1. Additional feature engineering and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8a327",
   "metadata": {},
   "source": [
    "As it was mentioned above, for better models performance it is necessary to perform additional feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7759d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Artist popularity (mean popularity of all tracks by that artist)\n",
    "df[\"artist_popularity\"] = df.groupby(\"artists\")[\"popularity\"].transform(\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Macro-genre from track_genre\n",
    "def map_macro_genre(g):\n",
    "    g = str(g).lower()\n",
    "    if \"pop\" in g:\n",
    "        return \"Pop\"\n",
    "    elif \"rock\" in g:\n",
    "        return \"Rock\"\n",
    "    elif \"hip hop\" in g or \"rap\" in g or \"trap\" in g:\n",
    "        return \"Hip-Hop/Rap\"\n",
    "    elif \"r&b\" in g or \"soul\" in g:\n",
    "        return \"R&B/Soul\"\n",
    "    elif \"electro\" in g or \"techno\" in g or \"house\" in g or \"edm\" in g or \"dance\" in g:\n",
    "        return \"Electronic/Dance\"\n",
    "    elif \"metal\" in g or \"hardcore\" in g:\n",
    "        return \"Metal/Hardcore\"\n",
    "    elif \"jazz\" in g or \"blues\" in g:\n",
    "        return \"Jazz/Blues\"\n",
    "    elif \"classical\" in g or \"orchestra\" in g or \"piano\" in g:\n",
    "        return \"Classical\"\n",
    "    elif \"latin\" in g or \"reggaeton\" in g or \"sertanejo\" in g or \"samba\" in g:\n",
    "        return \"Latin\"\n",
    "    elif \"country\" in g:\n",
    "        return \"Country\"\n",
    "    elif \"folk\" in g or \"singer-songwriter\" in g:\n",
    "        return \"Folk\"\n",
    "    elif \"indie\" in g or \"alternative\" in g:\n",
    "        return \"Indie/Alternative\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "df[\"macro_genre\"] = df[\"track_genre\"].apply(map_macro_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Interaction features\n",
    "df[\"energy_valence\"] = df[\"energy\"] * df[\"valence\"]              # mood-like interaction\n",
    "df[\"loudness_danceability\"] = df[\"loudness\"] * df[\"danceability\"] # intense + danceable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117559a",
   "metadata": {},
   "source": [
    "Select features (drop key, mode, time_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And select our features for future models\n",
    "\n",
    "\n",
    "target = \"popularity\"\n",
    "\n",
    "numeric_features = [\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"loudness\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\",\n",
    "    \"duration_min\",\n",
    "    \"artist_popularity\",\n",
    "    \"energy_valence\",\n",
    "    \"loudness_danceability\",\n",
    "]\n",
    "\n",
    "# key, mode, time_signature are intentionally NOT included (removed as low-value)\n",
    "\n",
    "categorical_features = [\"explicit\", \"macro_genre\"]\n",
    "\n",
    "# Prepare data\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df[target]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb716c65",
   "metadata": {},
   "source": [
    "Preprocessor (no scaling for trees, just one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fa3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing: scaling + one-hot encoding\n",
    "numeric_transformer = \"passthrough\"  # trees don't need scaling\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\") # one-hot encode categorical features\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(               \n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267693dd",
   "metadata": {},
   "source": [
    "##### 2.2 Random Forrest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ca10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Random Forrest Model\n",
    "\n",
    "rf_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"regressor\", RandomForestRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=None,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "# Fit model\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0649c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(rf_model, '../model/spotify_rf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print(\"Random Forest Results\")\n",
    "print(f\"MAE  : {rf_mae:.3f}\")\n",
    "print(f\"RMSE : {rf_rmse:.3f}\")\n",
    "print(f\"R^2  : {rf_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_features = rf_model.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "rf_importances = rf_model.named_steps[\"regressor\"].feature_importances_\n",
    "\n",
    "rf_importance_df = pd.DataFrame({\n",
    "    \"feature\": rf_features,\n",
    "    \"importance\": rf_importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "rf_importance_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45508638",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "top_rf = rf_importance_df.head(20)\n",
    "plt.bar(top_rf[\"feature\"], top_rf[\"importance\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Random Forest Feature Importance (Top 20)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4d968",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Random Forest significantly improves predictive performance compared to Linear Regression.  \n",
    "It captures **nonlinear relationships** and **feature interactions** between audio characteristics and popularity.  \n",
    "\n",
    "Feature importance becomes more interpretable: macro-genres, mood-related interactions, loudness, danceability, and acousticness have strong influence.  \n",
    "Random Forest is robust, less sensitive to outliers, and provides a strong benchmark for more advanced models like XGBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c06f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regressor\n",
    "\n",
    "\n",
    "xgb_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"regressor\", XGBRegressor(\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            objective=\"reg:squarederror\"\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "# Fit model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "print(\"XGBoost Results\")\n",
    "print(f\"MAE  : {xgb_mae:.3f}\")\n",
    "print(f\"RMSE : {xgb_rmse:.3f}\")\n",
    "print(f\"R^2  : {xgb_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586dae8e",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1132aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(xgb_model, '../model/spotify_xgb.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1da46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "xgb_features = xgb_model.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "xgb_importances = xgb_model.named_steps[\"regressor\"].feature_importances_\n",
    "\n",
    "# Build importance dataframe\n",
    "xgb_importance_df = pd.DataFrame({\n",
    "    \"feature\": xgb_features,\n",
    "    \"importance\": xgb_importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "xgb_importance_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot XGBoost feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_xgb = xgb_importance_df.head(20)\n",
    "plt.bar(top_xgb[\"feature\"], top_xgb[\"importance\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"XGBoost Feature Importance (Top 20)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee92d4",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "XGBoost delivers the best performance among all tested models.  \n",
    "It uses gradient boosting to iteratively improve predictions and can model complex, nonlinear relationships in the data.  \n",
    "\n",
    "XGBoost handles the engineered features particularly well, boosting performance on track-age, artist popularity, macro-genres, and mood-based interactions.  \n",
    "This model provides the highest accuracy and best generalization, making it the preferred approach for popularity prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038010b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
